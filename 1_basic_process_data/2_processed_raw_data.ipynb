{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a82ba84",
   "metadata": {},
   "source": [
    "# Limpieza data raw \"correcta\"\n",
    "\n",
    "- Se toma la data raw \"correcta\", la data preprocessed, que como dice el nombre es la data raw correguido valores nulos por fallas de conexión con PI, setear segundos a cero, etc.\n",
    "\n",
    "- **Limpiezas hechas**:\n",
    "    - Suavizar\n",
    "    - Timegap\n",
    "    - Pivot data\n",
    "    - Eliminar PGP\n",
    "\n",
    "    \n",
    "- **Cambios con respecto a la limpieza ORIGINAL DEL RECOMENDADOR**:\n",
    "    - Se suaviza solo con 2 periodos como ventanas en lugar de 5\n",
    "    - Se utiliza el valor promedio del timegap para suavizar en lugar de un timegap dinámico (simplicar). Punto de observación al inicio del proceso\n",
    "    - Se guarda en una columna el valor actual de tag y en otra columna el tag desplazado (CREACIÓN NUEVA NO EXISTENTE ANTES)\n",
    "\n",
    "-------\n",
    "**DATA**:\n",
    "- INPUT: \"data_raw_preprocessed.pkl\"\n",
    "- OUTPUT: \"data_raw_processed.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d92244-23e9-46d3-b9d2-155e140b5d19",
   "metadata": {},
   "source": [
    "## Root folder and read env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d542e20-ba04-4c72-b5b6-8bdc3101d6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root path:  D:\\github-mi-repo\\Optimization-Industrial-Process\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# fix root path to save outputs\n",
    "actual_path = os.path.abspath(os.getcwd())\n",
    "list_root_path = actual_path.split('\\\\')[:-1]\n",
    "root_path = '\\\\'.join(list_root_path)\n",
    "os.chdir(root_path)\n",
    "print('root path: ', root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3ec078-792a-408f-af8c-128ad4e31adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv # package used in jupyter notebook to read the variables in file .env\n",
    "\n",
    "\"\"\" get env variable from .env \"\"\"\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\"\"\" Read env variables and save it as python variable \"\"\"\n",
    "PROJECT_GCP = os.environ.get(\"PROJECT_GCP\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb59fec-8a93-48e8-aa3c-7ab3c2d88f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bab3cfcf-5a89-44ca-a2fd-52b9870675cd",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfccca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gcsfs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b5680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4eae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec560d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "748d224d",
   "metadata": {},
   "source": [
    "### 0. Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437d587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones auxiliares\n",
    "def load_all_parameterstags_tagclassification(model_name):\n",
    "    \"\"\"\n",
    "    Read a dictionary with all parameters filtered by model (d0eop, d1d2, d2, etc) located in TagClassification \n",
    "    \"\"\"\n",
    "    path_json = 'config/params.json'\n",
    "    with open(\"{path}\".format(path=path_json)) as json_file:\n",
    "        tag_classification_pars = json.load(json_file)\n",
    "\n",
    "    return tag_classification_pars[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6156ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define name of model - transversal model for this example\n",
    "general_params_models = 'blanqueo_santafe_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5210237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c9e3bbc",
   "metadata": {},
   "source": [
    "### 1. Leer data raw - datalake\n",
    "### 1. Read data raw datalake - preprocessed\n",
    "- Data get in the previous notebook\n",
    "- Data without nulls - filled in previous step for problems in upload data, no conextion PI-datalake, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79fbc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Tag</th>\n",
       "      <th>PV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>230AIT446.PNT</td>\n",
       "      <td>11.55654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01 00:05:00</td>\n",
       "      <td>230AIT446.PNT</td>\n",
       "      <td>11.55354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01 00:10:00</td>\n",
       "      <td>230AIT446.PNT</td>\n",
       "      <td>11.55110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime            Tag        PV\n",
       "0 2021-01-01 00:00:00  230AIT446.PNT  11.55654\n",
       "1 2021-01-01 00:05:00  230AIT446.PNT  11.55354\n",
       "2 2021-01-01 00:10:00  230AIT446.PNT  11.55110"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_raw_data = f'artifacts/data/data_raw_preprocessed.pkl'\n",
    "basic_preprocessed_data = pd.read_pickle(path_raw_data)\n",
    "basic_preprocessed_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19d53f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primera fecha:  2021-01-01 00:00:00\n",
      "ultima fecha:  2023-01-02 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print('primera fecha: ', basic_preprocessed_data['datetime'].min())\n",
    "print('ultima fecha: ', basic_preprocessed_data['datetime'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9f532b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate number of nulls\n",
    "df_aux = basic_preprocessed_data.pivot(index='datetime', columns='Tag', values='PV')\n",
    "df_aux.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e676f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f6a0b4e",
   "metadata": {},
   "source": [
    "## Continuar los pasos de limpieza después de obtener data raw correcta sin problemas de valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b56227",
   "metadata": {},
   "source": [
    "### 5. Suavizar Data\n",
    "En el json está el parámetro de periodos que se utilizan para suavizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fcd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class smooth_data(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    Suavizar la data. \n",
    "    - Actualmente suavizar toda la data (excepto los tags de tiempo de retención) de acuerdo a un parámetro fijo\n",
    "    - Se suaviza cuando está toda la data con valores y NO HAY NULOS, así evito que se expandan los nulos al aplicar el rolling\n",
    "    \n",
    "    - Obs: recibe la data raw, pivotear, suaviza, vuelve\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, freq_rolling_smooth_data, timegap_tag_list):\n",
    "        super(smooth_data,self).__init__()\n",
    "        self.freq_rolling_smooth_data = freq_rolling_smooth_data # freq con la que se va suavizar\n",
    "        self.timegap_tag_list = timegap_tag_list # listado de tags de tiempos de retención\n",
    "\n",
    "    def fit(self,DataFrame):\n",
    "        return self\n",
    "\n",
    "    def transform(self,DataFrame):\n",
    "        print('\\napplying smooth_data...')\n",
    "        \n",
    "        # pivotear data\n",
    "        print('Tamaño data raw: ', DataFrame.shape)\n",
    "        DataFrame = DataFrame.pivot(index='datetime', columns='Tag', values='PV')\n",
    "\n",
    "        # definir tags que se van a suavizar -> todos excepto los tag de timegap (esos datos tienen su propio tratamiento en otra función)\n",
    "        list_tags_to_smooth = list(set(DataFrame.columns.tolist()) - set(self.timegap_tag_list ))\n",
    "        \n",
    "        # suavizar la data y borrar nulos generados\n",
    "        DataFrame[list_tags_to_smooth] = DataFrame[list_tags_to_smooth].rolling(self.freq_rolling_smooth_data).mean()\n",
    "        DataFrame = DataFrame.dropna()\n",
    "        print('Cantidad de datos a borrarse por suavizar la data(primeros valores): ', (self.freq_rolling_smooth_data - 1) * DataFrame.shape[1])\n",
    "        \n",
    "        \n",
    "        # unpivot de la data porque el timegap dinámico está armado en base a la estructura de la data raw\n",
    "        DataFrame = DataFrame.reset_index()\n",
    "        DataFrame = pd.melt(DataFrame, id_vars = ['datetime'])\n",
    "        DataFrame.rename(columns = {'value': 'PV'}, inplace = True)\n",
    "        print('Tamaño data luego de suavizar: ', DataFrame.shape)\n",
    "        \n",
    "        return DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df00ff07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANTE\n",
    "# ventana filtro ma - AHORA ES DE 2 PERIODOS COMO VENTANA (en lugar de 5 periodos como tiene la limpiezaz original)\n",
    "selected_model_name = 'blanqueo_santafe_all'\n",
    "tag_classification_pars = load_all_parameterstags_tagclassification(model_name = general_params_models)\n",
    "moving_average_all_tags = tag_classification_pars['cleaning_pars']['moving_average_all_data']\n",
    "moving_average_all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31691b3d-606c-4391-9a39-28af8fb8c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read master tag table\n",
    "path_MaestroEtapas = 'config/MaestroEtapas.xlsx'\n",
    "MaestroEtapas = pd.read_excel(path_MaestroEtapas)\n",
    "\n",
    "# list of tags wuith values of timegap - this tags has its own process of cleaning\n",
    "timegaptag_list = list(MaestroEtapas.dropna()['TAG_TIEMPO_RESIDENCIA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fb89a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "applying smooth_data...\n",
      "Tamaño data raw:  (12842269, 3)\n",
      "Cantidad de datos a borrarse por suavizar la data(primeros valores):  61\n",
      "Tamaño data luego de suavizar:  (12842208, 3)\n"
     ]
    }
   ],
   "source": [
    "# instancia de la clase\n",
    "smooter = smooth_data(freq_rolling_smooth_data = moving_average_all_tags, \n",
    "                      timegap_tag_list = timegaptag_list\n",
    "                     )\n",
    "\n",
    "# transform\n",
    "preprocessed_data = smooter.transform(basic_preprocessed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef8ab7",
   "metadata": {},
   "source": [
    "### 6. TimeGap PROMEDIO\n",
    "- En la limpieza original se utiliza los timegap dinámicos (se utilizan tiempos de residencia como función de la producción y el tamaño del tanque). Esto tiene diferentes problemas al generar muchos nulos\n",
    "\n",
    "- En este ejemplo sencillo, se obviará los timegap dinámicos y se utilizará un TIMEGAP PROMEDIO (donde este valor promedio se obtuvo en combinación con los tiempos de retención promedio que manejan los ingenieros de procesos y validado a través de los tags de tiempo de renteción obteniendo el promedio de cada uno de lo valores de tiempo de residencia)\n",
    "\n",
    "- Al utilizar simplemente un desplazamiento promedio no se generan nulos y es relativamente cercano a la realidad (por lo menos cuando el proceso es estable el cual cual debería ser la mayor parte del tiempo)\n",
    "\n",
    "- Para hacer los desplazamiento de la data me voy a parar AL INICIO DEL PROCESO en la etapa D0 (en una primera instancia no se va a considerar la etapa ácida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e16974e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2831aea3",
   "metadata": {},
   "source": [
    "#### 6.1 Auxiliar. Crear diferentes tags de producción para ser desplazados\n",
    "- Originalmente existen los tags de producción a la entrada de blanqueo (etapa A), entrada de eop y bypass.\n",
    "- Se van a crear tags de producción para la etapa D0, D1 y P (simplemente tomar la producción del blanqueo a la entrada y crear las variables de producción a la entrada de las etapas D0, D1 y P, las que posteriormente se desplazaran de acuerdo al tiempo de residencia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b92dfb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_tags_produccion_cada_etapa(df):\n",
    "    '''\n",
    "    Crear tag de producción en las etapas que no existe directamente el tag de producción: D0, D1, P\n",
    "    '''\n",
    "    # pivotear data\n",
    "    df_pivot = df.pivot(index='datetime', columns='Tag', values='PV')\n",
    "\n",
    "    \"\"\"\n",
    "    ######### DISCOVERY #########\n",
    "    ### DISCOVERY - DIFERENCIA ENTRE LOS VALORES DE (PRODUCCIÓN A LA ENTRADA DE BLANQUEO) VS (PRODUCCIÓN ENTRADA E0P + BYPASS)\n",
    "    ### DISCOVERY - SE OBSERVA QUE LA DIFERENCIA ENTRE AMBAS VOLARES RONDA ENTRE 20 A 70 ADT\n",
    "\n",
    "    list_tags_produccion = ['240FI020A.PNT', '240FI020B.PNT', '240FI108A.PNT']\n",
    "    df_pivot['prod_eop_bypass'] = df_pivot['240FI020B.PNT'] + df_pivot['240FI108A.PNT']\n",
    "    df_pivot['diff_prod_d0_eopbypass'] = df_pivot['240FI020A.PNT'] - df_pivot['prod_eop_bypass']\n",
    "    print('\\nDescribe diferencia')\n",
    "    print(df_pivot['diff_prod_d0_eopbypass'].describe(percentiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.90, 0.95]))\n",
    "    print('\\nHistograma diferencia')\n",
    "    df_pivot['diff_prod_d0_eopbypass'].hist()\n",
    "    \"\"\"\n",
    "\n",
    "    # crear tags de producción - aún no se desplaza la data - en el maestro tag está definido el tag con el nombre que se va a\n",
    "    # crear para desplazar los valores de producción a su torre correspondiente\n",
    "    df_pivot['calc_prod_d0'] = df_pivot['240FI020A.PNT']\n",
    "    df_pivot['calc_prod_d1'] = df_pivot['240FI020A.PNT']\n",
    "    df_pivot['calc_prod_p'] = df_pivot['240FI020A.PNT']\n",
    "\n",
    "    # unpivot\n",
    "    df_unpivot = pd.melt(df_pivot.reset_index(), id_vars = ['datetime'])\n",
    "    df_unpivot.columns = ['datetime', 'Tag', 'PV']\n",
    "    \n",
    "    return df_unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a358ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = crear_tags_produccion_cada_etapa(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8ec32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8855ec06",
   "metadata": {},
   "source": [
    "#### 6.2 Aplicar timegap promedio\n",
    "- Se necesita archivo: Maestro Etapas que tiene las etapas y sus tiempos de retención promedio\n",
    "- Se necesita archivo: Maestro Tags que tiene la clasificación de los tags en cada una de las etapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "519e8bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliar\n",
    "def get_list_stages_to_calculate_timegap(stage_start, MaestroEtapas):\n",
    "    '''\n",
    "    - Definir una lista de los stages que se van a considerar para aplicar timegap.\n",
    "    - El input contiene \"stage_start\" que indica desde qué stage se quiere considerar. Por ejemplo, si fuera D1, quiero solo\n",
    "    considerar los stage desde el D1 en adelante (incluyendolo).\n",
    "    '''\n",
    "    \n",
    "    # listado completo de stages desde el maestro etapas\n",
    "    list_all_stages = MaestroEtapas['ETAPA'].tolist()\n",
    "    \n",
    "    # obtener solo el listado de stages desde la etapa que me interesa\n",
    "    list_stages = []\n",
    "    mark = False # marca para indicar que se encontró la etapa desde la que se va a considerar timegap\n",
    "    for stage in list_all_stages:\n",
    "\n",
    "        # si se llega al stage desde que se quiere aplicar el timegap hacia adelante, la marca cambia a True\n",
    "        if stage == stage_start:\n",
    "            mark = True\n",
    "\n",
    "        # siolo si la marca es True, se considera guardar el stage en el listado de stages que se van a considerar\n",
    "        if mark == True:\n",
    "            list_stages.append(stage)\n",
    "            \n",
    "    return list_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b07b67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_timegap_mean(df):\n",
    "    '''\n",
    "    Aplicar el timegap Promedio a toda la data DESDE LA ETAPA ÁCIDA HACIA ADELANTE\n",
    "    '''\n",
    "    \n",
    "    \"\"\" pivotear data \"\"\"\n",
    "    df_pivot = df.pivot(index='datetime', columns='Tag', values='PV')\n",
    "\n",
    "    \"\"\" read MaestroTags \"\"\"\n",
    "    path_MaestroTags = f'config/MaestroTags.xlsx'\n",
    "    MaestroTags = pd.read_excel(path_MaestroTags)\n",
    "\n",
    "    \"\"\" read MaestroEtapas \"\"\"\n",
    "    path_MaestroEtapas = f'config/MaestroEtapas.xlsx'\n",
    "    MaestroEtapas = pd.read_excel(path_MaestroEtapas)\n",
    "\n",
    "    # obtener listado de stages para timegap\n",
    "    #list_stages_to_timegap = get_list_stages_to_calculate_timegap(stage_start = 'A', MaestroEtapas = MaestroEtapas)\n",
    "    list_stages_to_timegap = MaestroEtapas['ETAPA'].tolist()\n",
    "\n",
    "    \"\"\"\n",
    "    validar que todos los datos del dataframe estén en el maestro tags (asegurarse que todos los tags tiene asignado una etapa)\n",
    "    y filtrar maestro tags tenga solo los tags presentes en la data\n",
    "    \"\"\"\n",
    "    list_tags_data = df_pivot.columns.tolist()\n",
    "    list_tags_maestro_tags = MaestroTags['TAG'].tolist()\n",
    "    tags_not_in_maestro_tags = [tag for tag in list_tags_data if tag not in list_tags_maestro_tags]\n",
    "    if tags_not_in_maestro_tags == []:\n",
    "        print('Todos los datos del dataframe están en el maestro tags. Por lo tanto todos los tags tienen asignada una etapa')\n",
    "        MaestroTags = MaestroTags.query(f'TAG == {list_tags_data}')\n",
    "\n",
    "    \"\"\"\n",
    "    mover la data de acuerdo a los tiempos de residencia promedio\n",
    "    \"\"\"\n",
    "\n",
    "    # inicializar\n",
    "    timegap_acc = 0\n",
    "    df_final = pd.DataFrame(index = df_pivot.index)\n",
    "\n",
    "    # para cada uno de los stages...\n",
    "    for stage in list_stages_to_timegap:\n",
    "        print('\\n\\n')\n",
    "        print('CALCULANDO TIEMPOS DE RESIDENCIA ETAPA:', stage)\n",
    "        print('Timegap acumulado (observaciones): ', timegap_acc)\n",
    "\n",
    "\n",
    "        ############## DEFINIR LISTADO DE TAGS ##############\n",
    "        # filtrar el maestro tags solo por los tags que están en la etapa que se está calculando\n",
    "        list_tags_stage = MaestroTags[MaestroTags['ETAPA'] == stage]['TAG'].tolist()\n",
    "        print('Listado de tags del stage: ', list_tags_stage)\n",
    "\n",
    "        # listado con el sufijo \"_visto\" corresponden al valor del tag visto en la realidad\n",
    "        names_tag_visto = [tag + '_visto' for tag in df_pivot[list_tags_stage].columns]\n",
    "\n",
    "        # listado con el sufijo \"_real\" corresponden al valor del tag desplazado de acuerdo al tiempo de residencia\n",
    "        names_tag_real = [tag + '_real' for tag in df_pivot[list_tags_stage].columns]\n",
    "\n",
    "\n",
    "        ############## CREAR DATASET VISTO(DATA EN EL MOMENTO ACTUAL) Y DATASET REAL (DATA DESPLAZADA DE ACUERDO AL TIMEGAP) ##############\n",
    "        # data vista agregarle prefijo \"visto\"\n",
    "        df_pivot_visto = df_pivot[list_tags_stage].copy()\n",
    "        df_pivot_visto.columns = names_tag_visto\n",
    "\n",
    "        # data real agregarle prefijo \"real\"\n",
    "        df_pivot_real = df_pivot[list_tags_stage].copy()\n",
    "        df_pivot_real.columns = names_tag_real\n",
    "\n",
    "        # data real desplazarla de acuerdo al tiempo de residencia\n",
    "        df_pivot_real = df_pivot_real.shift(-timegap_acc)\n",
    "\n",
    "\n",
    "        ############## DATASET VISTO Y REAL GUARDARLOS Y TERMINAR ITERACIÓN DEL STAGE ACTUAL ##############\n",
    "        # agregar data real y data vista al dataframe final\n",
    "        df_final = df_final.merge(df_pivot_visto, left_index = True, right_index = True)\n",
    "        df_final = df_final.merge(df_pivot_real, left_index = True, right_index = True)\n",
    "\n",
    "        # calcular el valor de tiempo de residencia acumulado al terminar esta stage - en base a observaciones de 5 minutos\n",
    "        timegap_acc += int(MaestroEtapas[MaestroEtapas['ETAPA'] == stage]['TIEMPO_RESIDENCIA_PROMEDIO'].values[0] / 5)\n",
    "\n",
    "\n",
    "    df_final_unpivot = pd.melt(df_final.reset_index(), id_vars = ['datetime'])\n",
    "    df_final_unpivot.columns = ['datetime', 'Tag', 'PV']\n",
    "    return df_final_unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47b202a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los datos del dataframe están en el maestro tags. Por lo tanto todos los tags tienen asignada una etapa\n",
      "\n",
      "\n",
      "\n",
      "CALCULANDO TIEMPOS DE RESIDENCIA ETAPA: A\n",
      "Timegap acumulado (observaciones):  0\n",
      "Listado de tags del stage:  ['240FI020A.PNT', 'S220ALDP010', '230AIT446.PNT', '240LIT010.PNT', '240AIC022.MEAS', '240TIC023.MEAS', '240FY024A.RO01', '240FIC024.MEAS', '276CLO2_LGA.RO02', 'S76ALE017', '240FI020A_HRS_TORRE.C']\n",
      "\n",
      "\n",
      "\n",
      "CALCULANDO TIEMPOS DE RESIDENCIA ETAPA: D0\n",
      "Timegap acumulado (observaciones):  18\n",
      "Listado de tags del stage:  ['calc_prod_d0', '240AIT063B.PNT', '240AIT063A.PNT', 'S276PER002', 'SSTRIPPING015', '240FY050.RO02', '240FIC110.MEAS', '240FY039.RO01', '240FIC440.MEAS', '240FI020A_HRS_DO.C']\n",
      "\n",
      "\n",
      "\n",
      "CALCULANDO TIEMPOS DE RESIDENCIA ETAPA: EOP\n",
      "Timegap acumulado (observaciones):  20\n",
      "Listado de tags del stage:  ['240FI020B.PNT', '240TI139.PNT', '240AIC126.MEAS', '240FY11PB.RO01', '240FY118B.RO01', '240FY107A.RO01', '240FIC116.MEAS', '240FIC107.MEAS', '240FIC118.MEAS', 'S240ALDP031', 'S240ALDP032', '240FI020A_HRS_EOP.C']\n",
      "\n",
      "\n",
      "\n",
      "CALCULANDO TIEMPOS DE RESIDENCIA ETAPA: D1\n",
      "Timegap acumulado (observaciones):  30\n",
      "Listado de tags del stage:  ['240FI108A.PNT', 'calc_prod_d1', '240AIT225A.PNT', '240AIT225B.PNT', '240TIT223.PNT', '240AIC224.MEAS', '240FY218.RO02', '240FY212.RO01', '240FY210A.RO01', '240FIC210.MEAS', '240FIC212.MEAS', '240FI020A_HRS_D1.C']\n",
      "\n",
      "\n",
      "\n",
      "CALCULANDO TIEMPOS DE RESIDENCIA ETAPA: P\n",
      "Timegap acumulado (observaciones):  48\n",
      "Listado de tags del stage:  ['calc_prod_p', '240AIT322A.PNT', '240AIT322B.PNT', '240AIC324.MEAS', '240TIT323.PNT', '240FY397.RO01', '240FY312.RO01', '240FIC310.MEAS', '240FY318.RO02', '240FIC236.MEAS', '240FIC312.MEAS', '240FIC397.MEAS', '240AIC286.MEAS', '240FI020A_HRS_P.C']\n",
      "\n",
      "\n",
      "\n",
      "CALCULANDO TIEMPOS DE RESIDENCIA ETAPA: TAC\n",
      "Timegap acumulado (observaciones):  66\n",
      "Listado de tags del stage:  ['240AIT416B.PNT', 'S240ALDP022', '240AIC433.MEAS', '240FY430.RO01']\n",
      "\n",
      "\n",
      "\n",
      "CALCULANDO TIEMPOS DE RESIDENCIA ETAPA: Secado\n",
      "Timegap acumulado (observaciones):  186\n",
      "Listado de tags del stage:  ['S2MAQUINAT07']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data = apply_timegap_mean(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9b2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0984ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6e8c069",
   "metadata": {},
   "source": [
    "### 7. Pivotear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fd21b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pivot(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    A class used to transform the data with tags in vertical to data with tags horizontal(in columns)\n",
    "    \n",
    "    Input:\n",
    "        original_datetime\tETAPA\tTag\t            PV\t        datetime\n",
    "    0\t2021-10-01 00:20:00\tEOP\t    240AIC126.MEAS\t11.199728\t2021-10-01 13:55:00\n",
    "    1\t2021-10-01 00:25:00\tEOP\t    240AIC126.MEAS\t11.210860\t2021-10-01 14:00:00\n",
    "    2\t2021-10-01 00:30:00\tEOP\t    240AIC126.MEAS\t11.217164\t2021-10-01 14:05:00\n",
    "    \n",
    "    Si se pivotea la data original (original_datetime), no existen valores nulos\n",
    "    Si se pivotea la data aplicado el timegap, si existen valores nulos\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Pivot,self).__init__()\n",
    "\n",
    "    def fit(self,DataFrame):\n",
    "        return self\n",
    "\n",
    "    def transform(self,DataFrame):\n",
    "        print('\\naplicando pivot...')\n",
    "        \n",
    "        # hacer trnasformacion\n",
    "        DataFrame = DataFrame.pivot_table(index = \"datetime\", columns = \"Tag\", values = \"PV\")\n",
    "        \n",
    "        #info\n",
    "        print('\\n Tamaño dataframe: ', DataFrame.shape)\n",
    "        print('\\nCantidad de nulos (estos nulos se generan a aplicar el timegap): ', DataFrame.isnull().sum())\n",
    "        print('\\n% de nulos (estos nulos se generan a aplicar el timegap): ', 100 * (DataFrame.isnull().sum() / DataFrame.shape[0]))\n",
    "        return DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc643943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "aplicando pivot...\n",
      "\n",
      " Tamaño dataframe:  (210528, 128)\n",
      "\n",
      "Cantidad de nulos (estos nulos se generan a aplicar el timegap):  Tag\n",
      "230AIT446.PNT_real       0\n",
      "230AIT446.PNT_visto      0\n",
      "240AIC022.MEAS_real      0\n",
      "240AIC022.MEAS_visto     0\n",
      "240AIC126.MEAS_real     20\n",
      "                        ..\n",
      "calc_prod_d0_visto       0\n",
      "calc_prod_d1_real       30\n",
      "calc_prod_d1_visto       0\n",
      "calc_prod_p_real        48\n",
      "calc_prod_p_visto        0\n",
      "Length: 128, dtype: int64\n",
      "\n",
      "% de nulos (estos nulos se generan a aplicar el timegap):  Tag\n",
      "230AIT446.PNT_real      0.00000\n",
      "230AIT446.PNT_visto     0.00000\n",
      "240AIC022.MEAS_real     0.00000\n",
      "240AIC022.MEAS_visto    0.00000\n",
      "240AIC126.MEAS_real     0.00950\n",
      "                         ...   \n",
      "calc_prod_d0_visto      0.00000\n",
      "calc_prod_d1_real       0.01425\n",
      "calc_prod_d1_visto      0.00000\n",
      "calc_prod_p_real        0.02280\n",
      "calc_prod_p_visto       0.00000\n",
      "Length: 128, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# instancia de la clase\n",
    "pivoter = Pivot()\n",
    "\n",
    "# transformar data\n",
    "preprocessed_data = pivoter.transform(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f331cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab664d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5c8e45a",
   "metadata": {},
   "source": [
    "### 8. Eliminar datos de PGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7a9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropMaintenanceDates(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    A class to delete the dates when the plant was not operative\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,MaintenanceDataFrame,beginDateCol,endDateCol,**kwargs):\n",
    "        super(DropMaintenanceDates,self).__init__()\n",
    "        self.MaintenanceDataFrame = MaintenanceDataFrame \n",
    "        self.beginDateCol = beginDateCol\n",
    "        self.endDateCol = endDateCol\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self,DataFrame):\n",
    "        return self\n",
    "\n",
    "    def transform(self,DataFrame):\n",
    "        print('\\naplicando limpiezas fechas de mantención - PGP ...')\n",
    "        print('Tamaño data: ', DataFrame.shape)\n",
    "        \n",
    "        for i, row in self.MaintenanceDataFrame.iterrows():\n",
    "            DropIndex=DataFrame.loc[row[self.beginDateCol]:row[self.endDateCol]].index\n",
    "            DataFrame.drop(DropIndex,inplace=True)\n",
    "            \n",
    "        #info\n",
    "        print('Tamaño data luego de borrar fechas de matención: ', DataFrame.shape)\n",
    "        print('\\n% de nulos hasta el momento: ', 100 * (DataFrame.isnull().sum() / DataFrame.shape[0]))\n",
    "        return DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25c38b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maintenance_df(beginning_date, ending_date):\n",
    "    maintenance_df = pd.DataFrame()\n",
    "    maintenance_df[\"beginDate\"] = beginning_date\n",
    "    maintenance_df[\"endDate\"] = ending_date\n",
    "    return maintenance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc4303b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parámetros\n",
    "tag_classification_pars = load_all_parameterstags_tagclassification(model_name = general_params_models)\n",
    "beginning_date = tag_classification_pars['maintenance_dates']['beginDate']\n",
    "ending_date =  tag_classification_pars['maintenance_dates']['endDate']\n",
    "maintenance_df = create_maintenance_df(beginning_date, ending_date)\n",
    "\n",
    "pars_drop_maintenance_dates = {\n",
    "        \"MaintenanceDataFrame\": maintenance_df,\n",
    "        \"beginDateCol\": \"beginDate\",\n",
    "        \"endDateCol\": \"endDate\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9817744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "aplicando limpiezas fechas de mantención - PGP ...\n",
      "Tamaño data:  (210528, 128)\n",
      "Tamaño data luego de borrar fechas de matención:  (197568, 128)\n",
      "\n",
      "% de nulos hasta el momento:  Tag\n",
      "230AIT446.PNT_real      0.000000\n",
      "230AIT446.PNT_visto     0.000000\n",
      "240AIC022.MEAS_real     0.000000\n",
      "240AIC022.MEAS_visto    0.000000\n",
      "240AIC126.MEAS_real     0.010123\n",
      "                          ...   \n",
      "calc_prod_d0_visto      0.000000\n",
      "calc_prod_d1_real       0.015185\n",
      "calc_prod_d1_visto      0.000000\n",
      "calc_prod_p_real        0.024295\n",
      "calc_prod_p_visto       0.000000\n",
      "Length: 128, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# instancia de la clase\n",
    "droper_maintenance_dates = DropMaintenanceDates(**pars_drop_maintenance_dates)\n",
    "\n",
    "# transformar dataframe\n",
    "processed_data = droper_maintenance_dates.transform(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7afd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37648e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f6efd3b",
   "metadata": {},
   "source": [
    "### 9. GUARDAR PKL PROCESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf0900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data pkl cloud\n",
    "path_raw_data_processed = 'artifacts/data/data_raw_processed.pkl'\n",
    "with open(path_raw_data_processed, \"wb\") as output:\n",
    "    pickle.dump(processed_data, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca009b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
